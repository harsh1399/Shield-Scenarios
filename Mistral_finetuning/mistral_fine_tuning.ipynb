{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7711cc88-ad11-4c24-989b-253f848a3412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForCausalLM,\n",
    "AutoTokenizer,\n",
    "BitsAndBytesConfig,\n",
    "HfArgumentParser,\n",
    "AutoTokenizer,\n",
    "TrainingArguments,\n",
    "Trainer,\n",
    "GenerationConfig)\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "interpreter_login(new_session=False)\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623650b4-7914-4b86-9618-d531bf452620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe05364f-6b3e-41fa-8aa7-0239d5ebcab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch,'float16')\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00af100a-31be-481b-b56e-b0ba7481cf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:24<00:00, 48.12s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3eb94ff-5d60-43fc-9205-2d753ffa3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side='left',\n",
    "    add_eos_token=True,\n",
    "    add_bos_token = True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01983580-001c-46c9-9fbf-c6f83736b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset samples: 568\n",
      "test dataset samples: 26\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import random\n",
    "with open('bt_dataset.json') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "test_samples = random.sample(range(len(dataset)),26)\n",
    "\n",
    "train_jsons = []\n",
    "test_jsons = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    if i in test_samples:\n",
    "        test_jsons.append(dataset[i])\n",
    "    else:\n",
    "        train_jsons.append(dataset[i])\n",
    "\n",
    "print(\"train dataset samples:\",len(train_jsons))\n",
    "print(\"test dataset samples:\",len(test_jsons))\n",
    "\n",
    "with open('bt_dataset_train.json','w') as f:\n",
    "    json.dump(train_jsons,f)\n",
    "\n",
    "with open('bt_dataset_test.json','w') as f:\n",
    "    json.dump(test_jsons,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0776b6d9-6c6a-4dc9-80e8-54c4009e2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 568 examples [00:00, 15217.72 examples/s]\n",
      "Generating train split: 26 examples [00:00, 6606.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 568\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset= load_dataset('json',data_files = 'bt_dataset_train.json')\n",
    "test_dataset = load_dataset('json',data_files = 'bt_dataset_test.json')\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4146af70-af60-405a-a331-f8f1f8d668b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    system_prompt = f\"<s>[INST]{sample['instruction']}\"\n",
    "    summary = f\"summary:{sample['input']}[/INST]\"\n",
    "    output = f\"output:{sample['output']}</s>\" if sample[\"output\"] else None\n",
    "    \n",
    "    parts = [part for part in [system_prompt,summary,output] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e71da5-5c14-46ce-889d-daec5573f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a5718b-6ba7-400b-8791-2e47aada9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def preprocess_batch(batch,tokenizer,max_length):\n",
    "    return tokenizer(batch['text'],max_length = max_length,truncation=True,padding=True,return_tensors = \"pt\")\n",
    "\n",
    "def preprocess_dataset(tokenizer,max_length, seed, dataset):\n",
    "    print(\"---preprocessing dataset---\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    _preprocessing_function = partial(preprocess_batch,max_length = max_length,tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['instruction','input','output','text'],\n",
    "    )\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    # dataset.set_format(\"pt\",columns=[\"input_ids\",\"attention_mask\"],output_all_columns=True) \n",
    "    # requires_grad=True\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a659ea0b-0af3-4380-baa3-470a73b8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 568/568 [00:00<00:00, 6364.84 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4151.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = get_max_length(original_model)\n",
    "prompt_train = train_dataset['train'].map(create_prompt_formats)\n",
    "prompt_test = test_dataset['train'].map(create_prompt_formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85fadfe5-2517-4012-84d4-eb15086f3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---preprocessing dataset---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 568/568 [00:03<00:00, 150.35 examples/s]\n",
      "Filter: 100%|██████████| 568/568 [00:05<00:00, 96.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 568\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = preprocess_dataset(tokenizer,max_length,20,train_dataset['train'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462c005a-8ca4-4cec-8ddf-0b2b8e70f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---preprocessing dataset---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 26/26 [00:00<00:00, 247.08 examples/s]\n",
      "Filter: 100%|██████████| 26/26 [00:00<00:00, 281.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 26\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = preprocess_dataset(tokenizer,max_length,20,test_dataset['train'])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87c9cd-31de-4ad0-89f7-a72d6745c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb70fd3-8b81-41b9-b029-d72ecdbfd420",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa4eafb-4cae-40bf-96d6-fdb66c1d04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "## lora config\n",
    "config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = [\n",
    "       \"gate_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"down_proj\",\n",
    "        \"up_proj\",\n",
    "        \"k_proj\",\n",
    "        \"q_proj\"\n",
    "    ],\n",
    "    bias = \"none\",\n",
    "    lora_dropout = 0.05,\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "original_model.gradient_checkpointing_enable()\n",
    "kbit_model = prepare_model_for_kbit_training(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e524b9-9a68-45d4-a54c-5ee6de1c15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(kbit_model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768e8115-f50e-415a-8bfd-d45e35d10251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2887563792882719\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acfb3676-4665-429d-956c-3b1b5e3be934",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate_model = accelerator.prepare_model(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed400231-8792-4fd4-b433-4be98207214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 11:15:05, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.448083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.366858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.333560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.311269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.313911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.339954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.335184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/default/workspace/new_finetune/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=350, training_loss=0.26029074123927526, metrics={'train_runtime': 40621.2409, 'train_samples_per_second': 0.069, 'train_steps_per_second': 0.009, 'total_flos': 1.7898626711519232e+18, 'train_loss': 0.26029074123927526, 'epoch': 4.929577464788732})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "output_dir = 'fine_tuned_mistral'\n",
    "finetuned_name = \"mistral-bt-finetuned\"\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    max_steps = 350,\n",
    "    learning_rate = 3e-4,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    logging_dir = \"./logs\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 50,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length = True,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "accelerate_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model = accelerate_model,\n",
    "    train_dataset = train,\n",
    "    eval_dataset = test,\n",
    "    args = peft_training_args,\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc49a7-180e-4bf7-8c8a-81af6caf37e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_finetuning",
   "language": "python",
   "name": "new_finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
